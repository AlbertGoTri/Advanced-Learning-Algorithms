{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955c859f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# for array computations and loading data\n",
    "import numpy as np\n",
    "\n",
    "# for building linear regression models and preparing data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# for building and training neural networks\n",
    "import tensorflow as tf\n",
    "\n",
    "# custom functions\n",
    "import utils\n",
    "\n",
    "# reduce display precision on numpy arrays\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# suppress warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2271c387",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset from the text file\n",
    "data = np.loadtxt('./data/data_w3_ex1.csv', delimiter=',')\n",
    "\n",
    "# Split the inputs and outputs into separate arrays\n",
    "x = data[:,0]\n",
    "y = data[:,1]\n",
    "\n",
    "# Convert 1-D arrays into 2-D because the commands later will require it\n",
    "x = np.expand_dims(x, axis=1)\n",
    "y = np.expand_dims(y, axis=1)\n",
    "\n",
    "print(f\"the shape of the inputs x is: {x.shape}\")\n",
    "print(f\"the shape of the targets y is: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5589d3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the entire dataset\n",
    "utils.plot_dataset(x=x, y=y, title=\"input vs. target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3039d8e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables: x_ and y_.\n",
    "x_train, x_, y_train, y_ = train_test_split(x, y, test_size=0.40, random_state=1)\n",
    "\n",
    "# Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
    "x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=1)\n",
    "\n",
    "# Delete temporary variables\n",
    "del x_, y_\n",
    "\n",
    "print(f\"the shape of the training set (input) is: {x_train.shape}\")\n",
    "print(f\"the shape of the training set (target) is: {y_train.shape}\\n\")\n",
    "print(f\"the shape of the cross validation set (input) is: {x_cv.shape}\")\n",
    "print(f\"the shape of the cross validation set (target) is: {y_cv.shape}\\n\")\n",
    "print(f\"the shape of the test set (input) is: {x_test.shape}\")\n",
    "print(f\"the shape of the test set (target) is: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f839e2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot_train_cv_test(x_train, y_train, x_cv, y_cv, x_test, y_test, title=\"input vs. target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66993fe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the class\n",
    "scaler_linear = StandardScaler()\n",
    "\n",
    "# Compute the mean and standard deviation of the training set then transform it\n",
    "X_train_scaled = scaler_linear.fit_transform(x_train)\n",
    "\n",
    "print(f\"Computed mean of the training set: {scaler_linear.mean_.squeeze():.2f}\")\n",
    "print(f\"Computed standard deviation of the training set: {scaler_linear.scale_.squeeze():.2f}\")\n",
    "\n",
    "# Plot the results\n",
    "utils.plot_dataset(x=X_train_scaled, y=y_train, title=\"scaled input vs. target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cbf95f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the class\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "linear_model.fit(X_train_scaled, y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc899ab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Feed the scaled training set and get the predictions\n",
    "yhat = linear_model.predict(X_train_scaled)\n",
    "\n",
    "# Use scikit-learn's utility function and divide by 2\n",
    "print(f\"training MSE (using sklearn function): {mean_squared_error(y_train, yhat) / 2}\")\n",
    "\n",
    "# for-loop implementation\n",
    "total_squared_error = 0\n",
    "\n",
    "for i in range(len(yhat)):\n",
    "    squared_error_i  = (yhat[i] - y_train[i])**2\n",
    "    total_squared_error += squared_error_i                                              \n",
    "\n",
    "mse = total_squared_error / (2*len(yhat))\n",
    "\n",
    "print(f\"training MSE (for-loop implementation): {mse.squeeze()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d315ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Scale the cross validation set using the mean and standard deviation of the training set\n",
    "X_cv_scaled = scaler_linear.transform(x_cv)\n",
    "\n",
    "print(f\"Mean used to scale the CV set: {scaler_linear.mean_.squeeze():.2f}\")\n",
    "print(f\"Standard deviation used to scale the CV set: {scaler_linear.scale_.squeeze():.2f}\")\n",
    "\n",
    "# Feed the scaled cross validation set\n",
    "yhat = linear_model.predict(X_cv_scaled)\n",
    "\n",
    "# Use scikit-learn's utility function and divide by 2\n",
    "print(f\"Cross validation MSE: {mean_squared_error(y_cv, yhat) / 2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bfff88",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the class to make polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# Compute the number of features and transform the training set\n",
    "X_train_mapped = poly.fit_transform(x_train)\n",
    "\n",
    "# Preview the first 5 elements of the new training set. Left column is `x` and right column is `x^2`\n",
    "# Note: The `e+<number>` in the output denotes how many places the decimal point should \n",
    "# be moved. For example, `3.24e+03` is equal to `3240`\n",
    "print(X_train_mapped[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1596587",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the class\n",
    "scaler_poly = StandardScaler()\n",
    "\n",
    "# Compute the mean and standard deviation of the training set then transform it\n",
    "X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
    "\n",
    "# Preview the first 5 elements of the scaled training set.\n",
    "print(X_train_mapped_scaled[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bcffae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the class\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_mapped_scaled, y_train )\n",
    "\n",
    "# Compute the training MSE\n",
    "yhat = model.predict(X_train_mapped_scaled)\n",
    "print(f\"Training MSE: {mean_squared_error(y_train, yhat) / 2}\")\n",
    "\n",
    "# Add the polynomial features to the cross validation set\n",
    "X_cv_mapped = poly.transform(x_cv)\n",
    "\n",
    "# Scale the cross validation set using the mean and standard deviation of the training set\n",
    "X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
    "\n",
    "# Compute the cross validation MSE\n",
    "yhat = model.predict(X_cv_mapped_scaled)\n",
    "print(f\"Cross validation MSE: {mean_squared_error(y_cv, yhat) / 2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd468bb6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize lists to save the errors, models, and feature transforms\n",
    "train_mses = []\n",
    "cv_mses = []\n",
    "models = []\n",
    "polys = []\n",
    "scalers = []\n",
    "\n",
    "# Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
    "for degree in range(1,11):\n",
    "    \n",
    "    # Add polynomial features to the training set\n",
    "    poly = PolynomialFeatures(degree, include_bias=False)\n",
    "    X_train_mapped = poly.fit_transform(x_train)\n",
    "    polys.append(poly)\n",
    "    \n",
    "    # Scale the training set\n",
    "    scaler_poly = StandardScaler()\n",
    "    X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
    "    scalers.append(scaler_poly)\n",
    "    \n",
    "    # Create and train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_mapped_scaled, y_train )\n",
    "    models.append(model)\n",
    "    \n",
    "    # Compute the training MSE\n",
    "    yhat = model.predict(X_train_mapped_scaled)\n",
    "    train_mse = mean_squared_error(y_train, yhat) / 2\n",
    "    train_mses.append(train_mse)\n",
    "    \n",
    "    # Add polynomial features and scale the cross validation set\n",
    "    X_cv_mapped = poly.transform(x_cv)\n",
    "    X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
    "    \n",
    "    # Compute the cross validation MSE\n",
    "    yhat = model.predict(X_cv_mapped_scaled)\n",
    "    cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
    "    cv_mses.append(cv_mse)\n",
    "    \n",
    "# Plot the results\n",
    "degrees=range(1,11)\n",
    "utils.plot_train_cv_mses(degrees, train_mses, cv_mses, title=\"degree of polynomial vs. train and CV MSEs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28613622",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get the model with the lowest CV MSE (add 1 because list indices start at 0)\n",
    "# This also corresponds to the degree of the polynomial added\n",
    "degree = np.argmin(cv_mses) + 1\n",
    "print(f\"Lowest CV MSE is found in the model with degree={degree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e08616",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Add polynomial features to the test set\n",
    "X_test_mapped = polys[degree-1].transform(x_test)\n",
    "\n",
    "# Scale the test set\n",
    "X_test_mapped_scaled = scalers[degree-1].transform(X_test_mapped)\n",
    "\n",
    "# Compute the test MSE\n",
    "yhat = models[degree-1].predict(X_test_mapped_scaled)\n",
    "test_mse = mean_squared_error(y_test, yhat) / 2\n",
    "\n",
    "print(f\"Training MSE: {train_mses[degree-1]:.2f}\")\n",
    "print(f\"Cross Validation MSE: {cv_mses[degree-1]:.2f}\")\n",
    "print(f\"Test MSE: {test_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ec22c3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Add polynomial features\n",
    "degree = 1\n",
    "poly = PolynomialFeatures(degree, include_bias=False)\n",
    "X_train_mapped = poly.fit_transform(x_train)\n",
    "X_cv_mapped = poly.transform(x_cv)\n",
    "X_test_mapped = poly.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5867d518",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Scale the features using the z-score\n",
    "scaler = StandardScaler()\n",
    "X_train_mapped_scaled = scaler.fit_transform(X_train_mapped)\n",
    "X_cv_mapped_scaled = scaler.transform(X_cv_mapped)\n",
    "X_test_mapped_scaled = scaler.transform(X_test_mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248c013c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize lists that will contain the errors for each model\n",
    "nn_train_mses = []\n",
    "nn_cv_mses = []\n",
    "\n",
    "# Build the models\n",
    "nn_models = utils.build_models()\n",
    "\n",
    "# Loop over the the models\n",
    "for model in nn_models:\n",
    "    \n",
    "    # Setup the loss and optimizer\n",
    "    model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "    )\n",
    "\n",
    "    print(f\"Training {model.name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        X_train_mapped_scaled, y_train,\n",
    "        epochs=300,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(\"Done!\\n\")\n",
    "\n",
    "    \n",
    "    # Record the training MSEs\n",
    "    yhat = model.predict(X_train_mapped_scaled)\n",
    "    train_mse = mean_squared_error(y_train, yhat) / 2\n",
    "    nn_train_mses.append(train_mse)\n",
    "    \n",
    "    # Record the cross validation MSEs \n",
    "    yhat = model.predict(X_cv_mapped_scaled)\n",
    "    cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
    "    nn_cv_mses.append(cv_mse)\n",
    "\n",
    "    \n",
    "# print results\n",
    "print(\"RESULTS:\")\n",
    "for model_num in range(len(nn_train_mses)):\n",
    "    print(\n",
    "        f\"Model {model_num+1}: Training MSE: {nn_train_mses[model_num]:.2f}, \" +\n",
    "        f\"CV MSE: {nn_cv_mses[model_num]:.2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f945142c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Select the model with the lowest CV MSE\n",
    "model_num = 3\n",
    "\n",
    "# Compute the test MSE\n",
    "yhat = nn_models[model_num-1].predict(X_test_mapped_scaled)\n",
    "test_mse = mean_squared_error(y_test, yhat) / 2\n",
    "\n",
    "print(f\"Selected Model: {model_num}\")\n",
    "print(f\"Training MSE: {nn_train_mses[model_num-1]:.2f}\")\n",
    "print(f\"Cross Validation MSE: {nn_cv_mses[model_num-1]:.2f}\")\n",
    "print(f\"Test MSE: {test_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e561bc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset from a text file\n",
    "data = np.loadtxt('./data/data_w3_ex2.csv', delimiter=',')\n",
    "\n",
    "# Split the inputs and outputs into separate arrays\n",
    "x_bc = data[:,:-1]\n",
    "y_bc = data[:,-1]\n",
    "\n",
    "# Convert y into 2-D because the commands later will require it (x is already 2-D)\n",
    "y_bc = np.expand_dims(y_bc, axis=1)\n",
    "\n",
    "print(f\"the shape of the inputs x is: {x_bc.shape}\")\n",
    "print(f\"the shape of the targets y is: {y_bc.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc26584",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot_bc_dataset(x=x_bc, y=y_bc, title=\"x1 vs. x2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a1a451",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables.\n",
    "x_bc_train, x_, y_bc_train, y_ = train_test_split(x_bc, y_bc, test_size=0.40, random_state=1)\n",
    "\n",
    "# Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
    "x_bc_cv, x_bc_test, y_bc_cv, y_bc_test = train_test_split(x_, y_, test_size=0.50, random_state=1)\n",
    "\n",
    "# Delete temporary variables\n",
    "del x_, y_\n",
    "\n",
    "print(f\"the shape of the training set (input) is: {x_bc_train.shape}\")\n",
    "print(f\"the shape of the training set (target) is: {y_bc_train.shape}\\n\")\n",
    "print(f\"the shape of the cross validation set (input) is: {x_bc_cv.shape}\")\n",
    "print(f\"the shape of the cross validation set (target) is: {y_bc_cv.shape}\\n\")\n",
    "print(f\"the shape of the test set (input) is: {x_bc_test.shape}\")\n",
    "print(f\"the shape of the test set (target) is: {y_bc_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769af28c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "\n",
    "# Initialize the class\n",
    "scaler_linear = StandardScaler()\n",
    "\n",
    "# Compute the mean and standard deviation of the training set then transform it\n",
    "x_bc_train_scaled = scaler_linear.fit_transform(x_bc_train)\n",
    "x_bc_cv_scaled = scaler_linear.transform(x_bc_cv)\n",
    "x_bc_test_scaled = scaler_linear.transform(x_bc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c958bdc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Sample model output\n",
    "probabilities = np.array([0.2, 0.6, 0.7, 0.3, 0.8])\n",
    "\n",
    "# Apply a threshold to the model output. If greater than 0.5, set to 1. Else 0.\n",
    "predictions = np.where(probabilities >= 0.5, 1, 0)\n",
    "\n",
    "# Ground truth labels\n",
    "ground_truth = np.array([1, 1, 1, 1, 1])\n",
    "\n",
    "# Initialize counter for misclassified data\n",
    "misclassified = 0\n",
    "\n",
    "# Get number of predictions\n",
    "num_predictions = len(predictions)\n",
    "\n",
    "# Loop over each prediction\n",
    "for i in range(num_predictions):\n",
    "    \n",
    "    # Check if it matches the ground truth\n",
    "    if predictions[i] != ground_truth[i]:\n",
    "        \n",
    "        # Add one to the counter if the prediction is wrong\n",
    "        misclassified += 1\n",
    "\n",
    "# Compute the fraction of the data that the model misclassified\n",
    "fraction_error = misclassified/num_predictions\n",
    "\n",
    "print(f\"probabilities: {probabilities}\")\n",
    "print(f\"predictions with threshold=0.5: {predictions}\")\n",
    "print(f\"targets: {ground_truth}\")\n",
    "print(f\"fraction of misclassified data (for-loop): {fraction_error}\")\n",
    "print(f\"fraction of misclassified data (with np.mean()): {np.mean(predictions != ground_truth)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606213f1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize lists that will contain the errors for each model\n",
    "nn_train_error = []\n",
    "nn_cv_error = []\n",
    "\n",
    "# Build the models\n",
    "models_bc = utils.build_models()\n",
    "\n",
    "# Loop over each model\n",
    "for model in models_bc:\n",
    "    \n",
    "    # Setup the loss and optimizer\n",
    "    model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "    )\n",
    "\n",
    "    print(f\"Training {model.name}...\")\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        x_bc_train_scaled, y_bc_train,\n",
    "        epochs=200,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(\"Done!\\n\")\n",
    "    \n",
    "    # Set the threshold for classification\n",
    "    threshold = 0.5\n",
    "    \n",
    "    # Record the fraction of misclassified examples for the training set\n",
    "    yhat = model.predict(x_bc_train_scaled)\n",
    "    yhat = tf.math.sigmoid(yhat)\n",
    "    yhat = np.where(yhat >= threshold, 1, 0)\n",
    "    train_error = np.mean(yhat != y_bc_train)\n",
    "    nn_train_error.append(train_error)\n",
    "\n",
    "    # Record the fraction of misclassified examples for the cross validation set\n",
    "    yhat = model.predict(x_bc_cv_scaled)\n",
    "    yhat = tf.math.sigmoid(yhat)\n",
    "    yhat = np.where(yhat >= threshold, 1, 0)\n",
    "    cv_error = np.mean(yhat != y_bc_cv)\n",
    "    nn_cv_error.append(cv_error)\n",
    "\n",
    "# Print the result\n",
    "for model_num in range(len(nn_train_error)):\n",
    "    print(\n",
    "        f\"Model {model_num+1}: Training Set Classification Error: {nn_train_error[model_num]:.5f}, \" +\n",
    "        f\"CV Set Classification Error: {nn_cv_error[model_num]:.5f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a665119b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Select the model with the lowest error\n",
    "model_num = 3\n",
    "\n",
    "# Compute the test error\n",
    "yhat = models_bc[model_num-1].predict(x_bc_test_scaled)\n",
    "yhat = tf.math.sigmoid(yhat)\n",
    "yhat = np.where(yhat >= threshold, 1, 0)\n",
    "nn_test_error = np.mean(yhat != y_bc_test)\n",
    "\n",
    "print(f\"Selected Model: {model_num}\")\n",
    "print(f\"Training Set Classification Error: {nn_train_error[model_num-1]:.4f}\")\n",
    "print(f\"CV Set Classification Error: {nn_cv_error[model_num-1]:.4f}\")\n",
    "print(f\"Test Set Classification Error: {nn_test_error:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
